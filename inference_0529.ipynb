{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt, numpy as np, pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "from centernet import CenterNet, decode\n",
    "\n",
    "from data import VOCDataset\n",
    "from losses import centernet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading the context of data_file ...: 550it [00:00, ?it/s]\n",
      "loading the context of data_file ...: 50it [00:00, ?it/s]\n",
      "loading the context of data_file ...: 50it [00:00, 50111.16it/s]\n",
      "loading the context of data_file ...: 550it [00:00, 551354.49it/s]\n"
     ]
    }
   ],
   "source": [
    "input_shape = (512, 512)\n",
    "backbone = 'resnet50'\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "buffer_size = batch_size * 5\n",
    "lr = 1e-2\n",
    "freeze = True\n",
    "finetune = True or freeze\n",
    "\n",
    "data_path = \"..\\\\..\\\\Centernet\\\\VOC2007\"\n",
    "train_file = \"..\\\\..\\\\Centernet\\\\VOC2007\\\\ImageSets\\\\ceph\\\\train.txt\"\n",
    "val_file = \"..\\\\..\\\\Centernet\\\\VOC2007\\\\ImageSets\\\\ceph\\\\val.txt\"\n",
    "test_file = \"..\\\\..\\\\Centernet\\\\VOC2007\\\\ImageSets\\\\ceph\\\\test.txt\"\n",
    "\n",
    "\n",
    "train_dataset_raw = VOCDataset(data_path, input_shape, train_file,\n",
    "                             batch_size, False)\n",
    "train_dataset = train_dataset_raw.load_dataset()\n",
    "\n",
    "\n",
    "val_dataset_raw = VOCDataset(data_path, input_shape, val_file,\n",
    "                             batch_size, False)\n",
    "val_dataset = val_dataset_raw.load_dataset()\n",
    "\n",
    "########\n",
    "test_dataset_raw = VOCDataset(data_path, input_shape, test_file,\n",
    "                             batch_size, False)\n",
    "test_dataset = test_dataset_raw.load_dataset()\n",
    "########\n",
    "\n",
    "vis_dataset_raw = VOCDataset(data_path, input_shape, train_file, 1, False)\n",
    "vis_dataset = vis_dataset_raw.load_dataset().repeat()\n",
    "\n",
    "steps_per_epoch = len(train_dataset_raw) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt_path = './logs/test/20220621-062329/ckpt.h5'\n",
    "ckpt_path = '../centernet-tf2/logs/best_epoch_weights.h5'\n",
    "\n",
    "model = CenterNet(train_dataset_raw.class_names,\n",
    "                      backbone_weights='imagenet',\n",
    "                      freeze=freeze,\n",
    "                      finetune=finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 121 layers into a model with 1 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5067e491bdaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m121\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\tkdal\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2209\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[0;32m   2210\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2211\u001b[1;33m         \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2213\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tkdal\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    681\u001b[0m   \u001b[0mlayer_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_layer_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m     raise ValueError('You are trying to load a weight file '\n\u001b[0m\u001b[0;32m    684\u001b[0m                      \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m                      \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 121 layers into a model with 1 layers."
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(121, 512, 512, 3))\n",
    "model.load_weights(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_pred, wh_pred, reg_pred = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_pred = decode(hm_pred, wh_pred, reg_pred, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_mse(arr, gt):\n",
    "    pnt_x = int((arr[0] + arr[2])/2)\n",
    "    pnt_y = int((arr[1] + arr[3])/2)\n",
    "\n",
    "    xmin,ymin,xmax,ymax = gt\n",
    "\n",
    "    gt_x = int((xmin + xmax)/2)\n",
    "    gt_y = int((ymin + ymax)/2)\n",
    "\n",
    "    return np.sqrt((gt_y-pnt_y)**2 + (gt_x-pnt_x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = detections_pred\n",
    "\n",
    "mse_result = []\n",
    "\n",
    "# ['Glabella', 'R3', 'Cervical point', 'Go-2 Ramus down', 'Naison', 'Pogonion']\n",
    "\n",
    "for i in range(50):\n",
    "    #topk_x1, topk_y1, topk_x2, topk_y2, scores, class_ids\n",
    "    Glabella = []\n",
    "    point_R3 = []\n",
    "    Nasion = []\n",
    "\n",
    "\n",
    "    for topk_x1, topk_y1, topk_x2, topk_y2, scores, class_ids in t[i]:\n",
    "\n",
    "        if(class_ids==0):\n",
    "            Glabella.append([topk_x1, topk_y1, topk_x2, topk_y2])\n",
    "\n",
    "        elif(class_ids==1):\n",
    "            point_R3.append([topk_x1, topk_y1, topk_x2, topk_y2])\n",
    "\n",
    "        elif(class_ids==3):\n",
    "            Nasion.append([topk_x1, topk_y1, topk_x2, topk_y2])\n",
    "\n",
    "    \n",
    "    #print(len(Glabella))\n",
    "    \n",
    "    Glabella = np.array(Glabella)\n",
    "    point_R3 = np.array(point_R3)\n",
    "    Nasion = np.array(Nasion)\n",
    "\n",
    "    # ground truth\n",
    "    Glabella_gt = val_dataset_raw[i][1][0]\n",
    "    point_R3_gt = val_dataset_raw[i][1][1]\n",
    "    Nasion_gt = val_dataset_raw[i][1][3]\n",
    "\n",
    "\n",
    "    Glabella_mse = point_mse(Glabella[0], Glabella_gt)\n",
    "    point_R3_mse = point_mse(point_R3[0], point_R3_gt)\n",
    "    Nasion_mse = point_mse(Nasion[0], Nasion_gt)\n",
    "\n",
    "    #print(Glabella_mse, point_R3_mse, Nasion_mse)\n",
    "\n",
    "\n",
    "    mse_result.append([i+1, np.round(Glabella_mse, 4), np.round(point_R3_mse, 4), np.round(Nasion_mse, 4)])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "d = datetime.datetime.now()\n",
    "today = f'{d.month}_{d.day}'\n",
    "\n",
    "print(today)\n",
    "\n",
    "# 이후 strftime() method 사용해볼 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_result_df = pd.DataFrame(mse_result, columns = ['Data_num', 'Glabella', 'R3', 'Nasion'], )\n",
    "mse_result_df = pd.DataFrame(mse_result, columns = ['Data_num', 'Glabella', 'R3', 'Go-2 Ramus down'], )\n",
    "\n",
    "mse_result_df.to_csv(f'./csv/mse_{today}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de54506a83f8ee6b284703bffbec36bf2dd70efd52509c97f7e6dc01858c1532"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
